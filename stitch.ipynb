{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f75f2d4-8c42-47ba-9bcb-c081d3c5cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b9b697-cd56-439e-b0f6-ddaf3aa587af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276cf30b-8a41-481f-9c56-a5a9905816af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news.csv')\n",
    "df = df[['Title', 'Authors', 'Article Text', 'Date Published']].dropna()\n",
    "docs = []\n",
    "for _, row in df.iterrows():\n",
    "    metadata = {\n",
    "        \"title\": row[\"Title\"],\n",
    "        \"authors\": row[\"Authors\"],\n",
    "        \"date_published\": row[\"Date Published\"]\n",
    "    }\n",
    "    docs.append(Document(page_content=row[\"Article Text\"], metadata=metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c76549a-de3a-4544-8f49-3d1187c52574",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "doc_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe988924-6327-47c1-bdc7-81161306caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "def format_articles(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def process_documents(docs):\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        # Clean and format the content\n",
    "        content = doc.page_content.strip()\n",
    "        \n",
    "        # Enhanced metadata\n",
    "        metadata = {\n",
    "            'raw_text_index': str(len(processed_docs)),\n",
    "            'title': doc.metadata.get('title', ''),\n",
    "            'authors': doc.metadata.get('authors', ''),\n",
    "            'date_published': str(doc.metadata.get('date_published', '')),\n",
    "            'summary': content[:200] + \"...\"\n",
    "        }\n",
    "        \n",
    "        new_doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        processed_docs.append(new_doc)\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb041ff-19fd-4309-9aea-38900394a099",
   "metadata": {},
   "source": [
    "## Basic LLM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3def9c11-2b2f-41a1-9c10-b64e796e4128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_432548/3074736950.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM, embeddings, and vector store\n",
    "def initialize_components():\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    \n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=\"env-news\",\n",
    "        embedding=embeddings,\n",
    "        pinecone_api_key=pinecone_api_key,\n",
    "        text_key=\"raw_text_index\"\n",
    "    )\n",
    "    \n",
    "    return llm, embeddings, vectorstore\n",
    "\n",
    "# Load components\n",
    "llm, embeddings, vectorstore = initialize_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3742952-fdc7-4e3c-b455-9b08f37bb6b5",
   "metadata": {},
   "source": [
    "## LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f3100b-2ca7-4527-b07a-ec436dea37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_model(model_path=\"./lora_model\"):\n",
    "    try:\n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "        lora_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        merged_model = lora_model.merge_and_unload()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "        merged_model.to(device)\n",
    "        return merged_model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LoRA model: {str(e)}\")\n",
    "        # Fallback to base model if LoRA model fails to load\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "        return model, tokenizer\n",
    "    \n",
    "lora_model, lora_tokenizer = load_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e0add7-cad5-4d8d-ba8f-8324e8f52b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summaries using LoRA model\n",
    "def generate_summary(model, tokenizer, article_text, max_length=130, min_length=30):\n",
    "    input_text = \"Summarize: \" + article_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180519f4-0597-478d-b9df-56fca2721c8d",
   "metadata": {},
   "source": [
    "## Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79f7e697-c5d1-4833-a0bc-f56fd04c3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "retriever = vectorstore2.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56eaa5aa-67df-4eec-86fb-b1150928e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data models for grading\n",
    "class RelevanceGrade(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "class HallucinationGrade(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "class AnswerQualityGrade(BaseModel):\n",
    "    \"\"\"Binary score to assess if the answer addresses the question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Create structured LLM graders\n",
    "doc_relevance_grader = llm.with_structured_output(RelevanceGrade, method=\"function_calling\")\n",
    "hallucination_check_grader = llm.with_structured_output(HallucinationGrade, method=\"function_calling\")\n",
    "answer_quality_grader = llm.with_structured_output(AnswerQualityGrade, method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f00a820-91ec-4ad5-82b4-cac7b92ed92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "relevance_check_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a grader assessing document relevance to a user question. \"\n",
    "     \"If the document contains related keywords or semantic meaning, grade it as 'yes'.\"),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "])\n",
    "\n",
    "hallucination_check_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You assess if an LLM-generated response is grounded in facts.\"),\n",
    "    (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "])\n",
    "\n",
    "answer_quality_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You assess if an LLM-generated response sufficiently answers a given question.\"),\n",
    "    (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "])\n",
    "\n",
    "query_rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Rewrite input questions for better vectorstore retrieval.\"),\n",
    "    (\"human\", \"Initial question: \\n\\n {question} \\n Reformulate an improved question.\"),\n",
    "])\n",
    "\n",
    "contextual_rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use retrieved context to generate an answer in three concise sentences.\"),\n",
    "    (\"human\", \"Question: {question}\\nContext: {context}\"),\n",
    "])\n",
    "\n",
    "summary_fusion_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that integrates document summaries to provide comprehensive answers. \"\n",
    "     \"Use the following summaries to answer the question. Focus on being accurate and concise.\"),\n",
    "    (\"human\", \"Question: {question}\\nSummaries: {summaries}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5c050f2-e13c-493b-a865-603d5ecf4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chains\n",
    "relevance_grading_chain = relevance_check_prompt | doc_relevance_grader\n",
    "hallucination_grading_chain = hallucination_check_prompt | hallucination_check_grader\n",
    "answer_quality_chain = answer_quality_prompt | answer_quality_grader\n",
    "query_rewriting_chain = query_rewrite_prompt | llm | StrOutputParser()\n",
    "rag_response_chain = contextual_rag_prompt | llm | StrOutputParser()\n",
    "summary_fusion_chain = summary_fusion_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5c9d7c6-e582-4f17-83c8-1360f8f36492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph state\n",
    "class AgentWorkflowState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our multi-agent RAG workflow.\n",
    "    \"\"\"\n",
    "    rewritten_question: str\n",
    "    original_question: str\n",
    "    final_response: str\n",
    "    retrieved_documents: List\n",
    "    generated_summaries: List\n",
    "    retry_count: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e994aba2-9d68-4ab3-9b19-3d4454dadc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nodes (agents)\n",
    "\n",
    "def retrieve_articles(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"rewritten_question\"]\n",
    "\n",
    "    # Initial document retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    if not documents:\n",
    "        print(\"No documents retrieved. Trying vectorstore fallback.\")\n",
    "        documents = retriever.vectorstore.similarity_search(question, k=5)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": documents,\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": state[\"original_question\"],\n",
    "        \"retry_count\": state.get(\"retry_count\", 0),\n",
    "        \"generated_summaries\": state.get(\"generated_summaries\", []),\n",
    "        \"final_response\": state.get(\"final_response\", \"\")\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_retrieved_articles(state: Dict) -> Dict:\n",
    "    question = state[\"rewritten_question\"]\n",
    "    original_question = state[\"original_question\"]\n",
    "    documents = state[\"retrieved_documents\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    filtered_documents = []\n",
    "    for doc in documents:\n",
    "        if retry_count >= 3:\n",
    "            filtered_documents = documents\n",
    "            break\n",
    "        score = relevance_grading_chain.invoke({\"question\": original_question, \"document\": doc.page_content})\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            filtered_documents.append(doc)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": filtered_documents,\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": original_question,\n",
    "        \"retry_count\": retry_count,\n",
    "        \"final_response\": state.get(\"final_response\", \"\"),\n",
    "        \"generated_summaries\": state.get(\"generated_summaries\", [])\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_with_lora(state: Dict) -> Dict:\n",
    "    question = state[\"rewritten_question\"]\n",
    "    documents = state[\"retrieved_documents\"]\n",
    "    summaries = [generate_summary(lora_model, lora_tokenizer, doc.page_content) for doc in documents]\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": documents,\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": state[\"original_question\"],\n",
    "        \"generated_summaries\": summaries,\n",
    "        \"retry_count\": state.get(\"retry_count\", 0),\n",
    "        \"final_response\": state.get(\"final_response\", \"\")\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_final_response(state: Dict) -> Dict:\n",
    "    summaries_text = \"\\n\\n\".join(state[\"generated_summaries\"])\n",
    "    final_response = summary_fusion_chain.invoke({\n",
    "        \"summaries\": summaries_text,\n",
    "        \"question\": state[\"original_question\"]\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": state[\"retrieved_documents\"],\n",
    "        \"rewritten_question\": state[\"rewritten_question\"],\n",
    "        \"original_question\": state[\"original_question\"],\n",
    "        \"generated_summaries\": state[\"generated_summaries\"],\n",
    "        \"retry_count\": state[\"retry_count\"],\n",
    "        \"final_response\": final_response\n",
    "    }\n",
    "\n",
    "\n",
    "def rewrite_user_query(state: Dict) -> Dict:\n",
    "    retry_count = state.get(\"retry_count\", 0) + 1\n",
    "\n",
    "    if retry_count >= 5:\n",
    "        fallback_response = (\n",
    "            f\"I've searched for information about '{state['original_question']}', \"\n",
    "            f\"but couldn't find highly relevant documents. Here's a limited response.\"\n",
    "        )\n",
    "        return {\n",
    "            \"retrieved_documents\": state[\"retrieved_documents\"],\n",
    "            \"rewritten_question\": state[\"rewritten_question\"],\n",
    "            \"original_question\": state[\"original_question\"],\n",
    "            \"retry_count\": retry_count,\n",
    "            \"final_response\": fallback_response,\n",
    "            \"generated_summaries\": state.get(\"generated_summaries\", [])\n",
    "        }\n",
    "\n",
    "    new_question = query_rewriting_chain.invoke({\"question\": state[\"original_question\"]})\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": state[\"retrieved_documents\"],\n",
    "        \"rewritten_question\": new_question,\n",
    "        \"original_question\": state[\"original_question\"],\n",
    "        \"retry_count\": retry_count,\n",
    "        \"final_response\": state.get(\"final_response\", \"\"),\n",
    "        \"generated_summaries\": state.get(\"generated_summaries\", [])\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_fallback_response(state: Dict) -> Dict:\n",
    "    question = state[\"rewritten_question\"]\n",
    "    original_question = state[\"original_question\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    documents = vectorstore.similarity_search(question, k=3)\n",
    "    summaries = [generate_summary(lora_model, lora_tokenizer, doc.page_content) for doc in documents]\n",
    "    summaries_text = \"\\n\\n\".join(summaries)\n",
    "\n",
    "    final_response = summary_fusion_chain.invoke({\n",
    "        \"summaries\": summaries_text,\n",
    "        \"question\": original_question\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": documents,\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": original_question,\n",
    "        \"retry_count\": retry_count,\n",
    "        \"final_response\": final_response,\n",
    "        \"generated_summaries\": summaries\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9911e48-eb83-43a2-8e8b-cd0c482c9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edge functions\n",
    "\n",
    "def decide_to_generate_response(state: Dict) -> str:\n",
    "    \"\"\"Decide whether to generate an answer or transform the question.\"\"\"\n",
    "    filtered_documents = state[\"retrieved_documents\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    if retry_count >= 3:\n",
    "        return \"force_generate\"\n",
    "    elif not filtered_documents:\n",
    "        return \"not_relevant\"\n",
    "    else:\n",
    "        return \"relevant\"\n",
    "\n",
    "\n",
    "def evaluate_final_response(state: Dict) -> str:\n",
    "    \"\"\"Check if the final response is grounded and answers the question.\"\"\"\n",
    "    original_question = state[\"original_question\"]\n",
    "    documents = state[\"retrieved_documents\"]\n",
    "    response = state[\"final_response\"]\n",
    "\n",
    "    # Format docs as string for hallucination checking\n",
    "    document_context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    # Step 1: Check grounding (hallucination)\n",
    "    hallucination_check = hallucination_grading_chain.invoke({\n",
    "        \"documents\": document_context,\n",
    "        \"generation\": response\n",
    "    })\n",
    "\n",
    "    if hallucination_check.binary_score.lower() == \"yes\":\n",
    "        # Step 2: Check if it answers the question\n",
    "        answer_check = answer_quality_chain.invoke({\n",
    "            \"question\": original_question,\n",
    "            \"generation\": response\n",
    "        })\n",
    "\n",
    "        if answer_check.binary_score.lower() == \"yes\":\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            return \"not_useful\"\n",
    "    else:\n",
    "        return \"not_supported\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a3b29-8e3c-48d9-994f-b93c92050bc9",
   "metadata": {},
   "source": [
    "### With LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2672725-7277-4a87-9e16-9f1aafab81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "def build_agentic_rag_workflow():\n",
    "    workflow = StateGraph(AgentWorkflowState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"retrieve_articles\", retrieve_articles)\n",
    "    workflow.add_node(\"grade_retrieved_articles\", grade_retrieved_articles)\n",
    "    workflow.add_node(\"summarize_with_lora\", summarize_with_lora)\n",
    "    workflow.add_node(\"generate_final_response\", generate_final_response)\n",
    "    workflow.add_node(\"rewrite_user_query\", rewrite_user_query)\n",
    "    workflow.add_node(\"generate_fallback_response\", generate_fallback_response)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"retrieve_articles\")\n",
    "    workflow.add_edge(\"retrieve_articles\", \"grade_retrieved_articles\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_retrieved_articles\",\n",
    "        decide_to_generate_response,\n",
    "        {\n",
    "            \"not_relevant\": \"rewrite_user_query\",\n",
    "            \"relevant\": \"summarize_with_lora\",\n",
    "            \"force_generate\": \"generate_fallback_response\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"summarize_with_lora\", \"generate_final_response\")\n",
    "    workflow.add_edge(\"generate_fallback_response\", END)\n",
    "    workflow.add_edge(\"rewrite_user_query\", \"retrieve_articles\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_final_response\",\n",
    "        evaluate_final_response,\n",
    "        {\n",
    "            \"not_supported\": \"generate_final_response\",\n",
    "            \"useful\": END,\n",
    "            \"not_useful\": \"rewrite_user_query\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Compile\n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1a578cb-a212-4ca4-85b1-33231ddf6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rag_app = build_agentic_rag_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c23cc6-a808-43e5-b3b8-f69d181bc7e6",
   "metadata": {},
   "source": [
    "### With Base LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff7b62ab-f201-4377-b3a3-a9598299daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_base_llm(state: Dict) -> Dict:\n",
    "    \"\"\"Generates summaries using the base LLM instead of the LoRA fine-tuned model.\"\"\"\n",
    "    question = state[\"rewritten_question\"]\n",
    "    original_question = state[\"original_question\"]\n",
    "    documents = state[\"retrieved_documents\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    summaries = []\n",
    "    for doc in documents:\n",
    "        summary_prompt = f\"Summarize the following text in a concise manner: {doc.page_content}\"\n",
    "        summary = llm.invoke(summary_prompt).content\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_documents\": documents,\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": original_question,\n",
    "        \"generated_summaries\": summaries,\n",
    "        \"retry_count\": retry_count,\n",
    "        \"final_response\": state.get(\"final_response\", \"\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53872ef6-d02e-4389-9c6e-d7e35fe3e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified workflow using base LLM for summarization\n",
    "def build_base_llm_workflow():\n",
    "    workflow = StateGraph(AgentWorkflowState)\n",
    "\n",
    "    # Add nodes (same logic, base LLM replaces LoRA for summarization)\n",
    "    workflow.add_node(\"retrieve_articles\", retrieve_articles)\n",
    "    workflow.add_node(\"grade_retrieved_articles\", grade_retrieved_articles)\n",
    "    workflow.add_node(\"summarize_with_base_llm\", summarize_with_base_llm)  # <--- base model agent\n",
    "    workflow.add_node(\"generate_final_response\", generate_final_response)\n",
    "    workflow.add_node(\"rewrite_user_query\", rewrite_user_query)\n",
    "    workflow.add_node(\"generate_fallback_response\", generate_fallback_response)\n",
    "\n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"retrieve_articles\")\n",
    "    workflow.add_edge(\"retrieve_articles\", \"grade_retrieved_articles\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_retrieved_articles\",\n",
    "        decide_to_generate_response,\n",
    "        {\n",
    "            \"not_relevant\": \"rewrite_user_query\",\n",
    "            \"relevant\": \"summarize_with_base_llm\",\n",
    "            \"force_generate\": \"generate_fallback_response\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"summarize_with_base_llm\", \"generate_final_response\")\n",
    "    workflow.add_edge(\"generate_fallback_response\", END)\n",
    "    workflow.add_edge(\"rewrite_user_query\", \"retrieve_articles\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_final_response\",\n",
    "        evaluate_final_response,\n",
    "        {\n",
    "            \"not_supported\": \"generate_final_response\",\n",
    "            \"useful\": END,\n",
    "            \"not_useful\": \"rewrite_user_query\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Compile\n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10e7455c-375d-427b-b573-1f38b0f366a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the base workflow\n",
    "base_rag_app = build_base_llm_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "695f281a-0a64-4027-ae9c-83329e1ce1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic RAG chain for comparison\n",
    "def build_basic_rag_chain():\n",
    "    basic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    rag_prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    basic_rag_chain = (\n",
    "        {\"context\": basic_retriever | format_articles, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return basic_rag_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676b794-9f4a-4571-a0d0-695c579091ea",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2696144-35e6-44e3-9519-533cf3c6b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_system(question):\n",
    "    # Base LLM (no RAG)\n",
    "    base_llm_response = llm.invoke(question).content\n",
    "\n",
    "    # Basic RAG\n",
    "    basic_rag_chain = build_basic_rag_chain()\n",
    "    basic_rag_response = basic_rag_chain.invoke(question)\n",
    "\n",
    "    # Advanced RAG with base model (no fine-tuning)\n",
    "    base_inputs = {\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": question,\n",
    "        \"retry_count\": 0,\n",
    "        \"final_response\": \"\",\n",
    "        \"generated_summaries\": []\n",
    "    }\n",
    "    base_result = base_rag_app.invoke(base_inputs)\n",
    "    advanced_rag_base_response = base_result[\"final_response\"]\n",
    "\n",
    "    # Advanced RAG with LoRA model\n",
    "    lora_inputs = {\n",
    "        \"rewritten_question\": question,\n",
    "        \"original_question\": question,\n",
    "        \"retry_count\": 0,\n",
    "        \"final_response\": \"\",\n",
    "        \"generated_summaries\": []\n",
    "    }\n",
    "    lora_result = lora_rag_app.invoke(lora_inputs)\n",
    "    advanced_rag_lora_response = lora_result[\"final_response\"]\n",
    "\n",
    "    return {\n",
    "        \"base_llm\": base_llm_response,\n",
    "        \"basic_rag\": basic_rag_response,\n",
    "        \"advanced_rag_base\": advanced_rag_base_response,\n",
    "        \"advanced_rag_lora\": advanced_rag_lora_response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ab835-a0e2-49f8-8844-e36fc7a2fb0a",
   "metadata": {},
   "source": [
    "## Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0467ee99-e01c-4f16-b62b-330163b9e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/mpe9143/.local/lib/python3.9/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---RETRIEVE---\n",
      "\n",
      "====================================================================================================\n",
      "QUESTION:\n",
      "What are the latest developments in renewable energy covered in the news?\n",
      "====================================================================================================\n",
      "\n",
      "BASE LLM (No RAG)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1. Offshore wind farms are becoming increasingly popular as a source of renewable energy. The world's largest offshore wind farm, Hornsea One, recently opened off the coast of England, capable of powering over 1 million homes.\n",
      "\n",
      "2. Solar power continues to grow in popularity, with new advancements in solar panel technology making it more efficient and cost-effective. In India, the government recently announced plans to build the world's largest solar power plant in the state of Gujarat.\n",
      "\n",
      "3. The use of hydrogen as a clean energy source is gaining traction, with countries like Japan investing in hydrogen fuel cell technology for transportation and power generation. Toyota recently announced plans to build a hydrogen-powered city of the future in Japan.\n",
      "\n",
      "4. Geothermal energy is also seeing advancements, with new projects being developed in countries like Iceland and Kenya. Geothermal power plants harness heat from beneath the Earth's surface to generate electricity, providing a reliable and sustainable source of energy.\n",
      "\n",
      "5. The use of bioenergy, such as biofuels and biomass, is also on the rise as a renewable energy source. In the United States, companies are investing in bioenergy production from sources like agricultural waste and algae to reduce carbon emissions and dependence on fossil fuels.\n",
      "\n",
      "BASIC RAG\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I don't know.\n",
      "\n",
      "ADVANCED RAG (Base Model)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The latest developments in renewable energy include a 50% growth in renewable energy capacity in 2023, reaching 510 GW, setting a new record for the 22nd consecutive year according to the IEA. The importance of transitioning to renewable energy sources like wind, solar, and hydro power, along with energy conservation measures, has been emphasized for cost savings, environmental benefits, and contributing to a sustainable future. Additionally, transitioning away from fossil fuels is crucial for addressing climate change, with increased financing needed to support the transition in emerging and developing economies.\n",
      "\n",
      "ADVANCED RAG (LoRA Fine-Tuned Model)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The latest developments in renewable energy include a 50% growth in renewable energy capacity to 510 gigawatts in 2023, marking the 22nd consecutive year of record-setting additions, as reported by the IEA. The IEA's executive director, Fatih Birol, expressed enthusiasm about this significant growth in renewable energy.\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ques1 = \"What are the latest developments in renewable energy covered in the news?\"\n",
    "result1 = evaluate_system(ques1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"QUESTION:\\n{ques1}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nBASE LLM (No RAG)\")\n",
    "print(\"-\" * 100)\n",
    "print(result1[\"base_llm\"])\n",
    "\n",
    "print(\"\\nBASIC RAG\")\n",
    "print(\"-\" * 100)\n",
    "print(result1[\"basic_rag\"])\n",
    "\n",
    "print(\"\\nADVANCED RAG (Base Model)\")\n",
    "print(\"-\" * 100)\n",
    "print(result1[\"advanced_rag_base\"])\n",
    "\n",
    "print(\"\\nADVANCED RAG (LoRA Fine-Tuned Model)\")\n",
    "print(\"-\" * 100)\n",
    "print(result1[\"advanced_rag_lora\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
